# llm-prompt-leak-framework
Experimental framework for evaluating prompt leakage attacks and defenses in LLMs.

# Prompt Leakage Evaluation Framework

This repository contains the experimental code used in a bachelor's thesis
on prompt leakage attacks and defenses in large language models.

## Contents

prompt_leak_framework.py: main experimental script for attack generation, detection, and leakage evaluation.
